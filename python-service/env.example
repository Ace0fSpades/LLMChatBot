# ============================================================================
# SERVER CONFIGURATION
# ============================================================================
HOST=0.0.0.0
PORT=5000
DEBUG=false

# Note: CORS is not needed here - this is an internal service
# Only the Go backend communicates with this service
# CORS is configured on the Go backend

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Model Selection:
# - For GPU (CUDA): Qwen/Qwen2.5-7B-Instruct with 4bit quantization
# - For CPU: Qwen/Qwen2.5-3B-Instruct with 8bit quantization or none
MODEL_NAME=Qwen/Qwen2.5-3B-Instruct

# HuggingFace Token (get from: https://huggingface.co/settings/tokens)
HUGGINGFACE_TOKEN=your_token_here

# Device: "cuda" (GPU) or "cpu"
DEVICE=cuda

# Quantization:
# - "none": Full precision (GPU: 14GB for 7B, CPU: 6-8GB for 3B)
# - "4bit": GPU only, ~4GB for 7B model
# - "8bit": GPU/CPU, ~7GB for 7B, ~3-4GB for 3B
QUANTIZATION=none

# Data Type:
# - "bfloat16": Recommended for GPU with Qwen models
# - "float16": Good for GPU, may not work on all CPUs
# - "float32": Safest for CPU, slower but more compatible
DTYPE=bfloat16

# ============================================================================
# GENERATION PARAMETERS
# ============================================================================

# Maximum tokens to generate per response
# GPU: 512-2048, CPU: 128-512
MAX_NEW_TOKENS=512

# Temperature: Controls randomness (0.0-1.0)
TEMPERATURE=0.6

# Top-p: Nucleus sampling threshold
TOP_P=0.9

# Repetition Penalty: Prevents repetition (1.0 = no penalty)
REPETITION_PENALTY=1.1

# Context Window: Maximum conversation history
# Qwen 2.5 supports up to 32K, but reduce for CPU (4096-8192)
CONTEXT_WINDOW=32768

# ============================================================================
# STREAMING CONFIGURATION
# ============================================================================

# Tokens per chunk
CHUNK_SIZE=10

# Delay between chunks (milliseconds)
DELAY_MS=50

# Token buffer size
BUFFER_SIZE=100

# ============================================================================
# BACKEND INTEGRATION
# ============================================================================

# URL of the Go backend service
BACKEND_URL=http://localhost:8080