# LLMChatBot
A modular platform for creating domain-specific dialog systems based on the hybrid RAG architecture and parametrically efficient learning

---

### **Разработка модульной платформы для создания предметно-ориентированных диалоговых систем на основе гибридной архитектуры RAG и параметрически-эффективного обучения**

---

### 1. Основная идея проекта

*   **Что разрабатывается:** Модульная SaaS-платформа (MvP), которая позволяет малым предприятиям создавать и внедрять собственных AI-ассистентов, специализирующихся на их конкретной предметной области (например, поддержка клиентов интернет-магазина, консультант по услугам юридической фирмы, внутренний помощник по корпоративным регламентам). Платформа использует гибридный подход, комбинируя **Retrieval-Augmented Generation (RAG)** для работы с актуальными данными и **Parameter-Efficient Fine-Tuning (PEFT, в частности LoRA)** для тонкой настройки поведения модели под специфику бизнеса.

*   **Решаемые проблемы:**
    *   **Высокий порог входа:** Малому бизнесу недоступны ресурсы и экспертиза для самостоятельной разработки и развертывания современных LLM.
    *   **"Галлюцинации" и нерелевантные ответы:** Базовые LLM часто дают неточные или общие ответы, не основанные на внутренних данных компании.
    *   **Статичность знаний:** LLM не знает о непубличной информации компании (база знаний, прайсы, документация).
    *   **Стоимость и сложность кастомизации:** Полная тонкая настройка больших моделей требует огромных вычислительных ресурсов. LoRA решает эту проблему.

*   **Целевая аудитория:**
    *   **Малый и средний бизнес (SMB):** Интернет-магазины, локальные сервисные компании, консультанты, образовательные платформы.
    *   **Технически неподготовленные владельцы бизнеса,** которые хотят внедрить AI без найма команды data-scientists.

*   **Роли и задачи в проекте:**
    *   **Владелец бизнеса / Менеджер:** Регистрируется в системе, создает "проект" ассистента, загружает данные (документы, FAQ, сайт), настраивает базовые параметры (тон, стиль ответов), интегрирует ассистента через API на свой сайт/в чат.
    *   **Система (Backend):**
        *   Обработка и индексация загруженных данных (векторизация).
        *   Обучение LoRA-адаптеров по запросу пользователя.
        *   Обработка входящих запросов: поиск релевантного контекста (RAG) и генерация ответа LLM с учетом LoRA-адаптеров.
        *   Предоставление API.

---

### 2. Анализ требований

*   **Ожидания пользователей:**
    *   Простота использования (low-code/no-code подход).
    *   Быстрое создание ассистента "в несколько кликов".
    *   Точные и релевантные ответы, основанные на предоставленных данных.
    *   Возможность кастомизации "голоса" и поведения ассистента.
    *   Простая интеграция (виджет для сайта, REST API).
    *   Конфиденциальность их данных.

*   **Функциональные требования:**
    1.  **Управление данными:**
        *   Загрузка документов (PDF, DOCX, TXT).
        *   Возможность указать URL сайта для краулинга.
        *   Просмотр и удаление загруженных документов.
    2.  **Управление ассистентами:**
        *   Создание, редактирование, удаление ассистента.
        *   Настройка базового промпта и параметров генерации.
        *   Запуск процесса тонкой настройки (LoRA) на основе загруженных данных.
    3.  **Диалоговый движок (RAG + LoRA):**
        *   Векторизация текста и поиск семантически близких чанков.
        *   Интеграция найденного контекста в промпт LLM.
        *   Применение обученного LoRA-адаптера во время инференса.
    4.  **API:**
        *   REST API endpoint для отправки сообщений ассистенту.
        *   API-ключи для аутентификации.
    5.  **Веб-интерфейс (Админ-панель):**
        *   Дашборд с проектами.
        *   Интерфейс для загрузки данных и настройки ассистентов.
        *   Чат-интерфейс для тестирования ассистента.

---

### 3. Архитектура проекта

```
+------------------------+     HTTP/gRPC     +-----------------------+
|   Клиент (Streamlit)   | <----------------> |    Backend API (Go)   |
|     Веб-интерфейс      |                    |                       |
+------------------------+                    +-----------------------+
                                                    |  |  |
                    (Управление) (Данные запроса) (Данные для индексации)
                                                    |  |  |
                                    +-----------------------------------+
                                    |               |               |
                                    v               v               v
+------------------------+    +-----------+    +---------+    +------------+
|   Python AI Service    |    | PostgreSQL|    | Chroma  |    |  Файловое  |
|   (Flask/FastAPI)      |    | (Metadata)|    | (Vector)|    |  хранилище |
+------------------------+    +-----------+    +---------+    +------------+
        |
        | (Внутренний вызов)
        v
+-----------------------------------------------------------------------+
|                          AI Core (Python)                             |
| +------------------+  +-----------------------+  +------------------+ |
| |   Embeddings     |  |       LLM Engine      |  |   LoRA Manager   | |
| | (sentence-transformers)|  (Llama 3 8B +  |  |    (PEFT lib)     | |
| | all-MiniLM-L6-v2 |  |    Transformers)     |  |                  | |
| +------------------+  +-----------------------+  +------------------+ |
|          |                   |                        |              |
|          | (Вектор запроса)  | (Контекст + Запрос)    | (Загружает   |
|          +------------------->                        |   адаптер)   |
|                             |                        |              |
|                             +<------------------------+              |
+-----------------------------------------------------------------------+
```

**Взаимодействие компонентов:**
1.  **Backend API** принимает запросы от веб-интерфейса и внешних клиентов.
2.  **Celery Worker** выполняет тяжелые задачи асинхронно: индексация новых документов в **Векторную БД** и обучение **LoRA-адаптеров**.
3.  При запросе к ассистенту, **Backend** сначала обращается к **Векторной БД** для поиска релевантного контекста (RAG).
4.  Затем **Backend** отправляет контекст и запрос пользователя в **LLM Engine**.
5.  **LLM Engine** загружает базовую модель и соответствующий **LoRA-адаптер** (если он есть для данного ассистента) и генерирует ответ.
6.  Ответ возвращается клиенту через **Backend API**.

---

### 4. Используемые технологии

*   **Бэкенд:** Python 3.10+
*   **Веб-фреймворк:** FastAPI (легковесный, асинхронный, автоматическая генерация OpenAPI docs) или Django.
*   **Работа с LLM:** Hugging Face `transformers`, `accelerate`, `bitsandbytes` (для 4-bit квантования).
*   **PEFT/LoRA:** Библиотека `peft` от Hugging Face.
*   **Векторная база данных:** Chroma (легкая, встраиваемая) или Qdrant (производительная, с облачным хостингом).
*   **Реляционная БД (для метаданных):** PostgreSQL.
*   **Очереди задач:** Celery + Redis (в качестве брокера).
*   **Инференс LLM:** Предпочтительно собственный хостинг для MvP (например, на базе VPS с GPU). Модель-кандидат: `Llama 3 8B Instruct` или `Mistral 7B Instruct`.
*   **Фронтенд (Админ-панель):** React/Next.js + TypeScript (или Vue.js).
*   **Хостинг:** VPS (например, Hetzner, DigitalOcean) с GPU (например, RTX 4090 24GB) или облачные инстансы (Lambda Labs, Paperspace).
*   **Контейнеризация:** Docker, Docker Compose.
*   **Безопасность:** JWT-токены, хеширование паролей (bcrypt).

---

### 5. Вопросы безопасности

*   **Аутентификация и авторизация:** JWT-токены с ограниченным временем жизни. Strict access control: пользователь имеет доступ только к своим проектам и данным.
*   **Защита данных:**
    *   **Шифрование:** Все данные шифруются **на rest** (шифрование диска на сервере) и **на лету** (HTTPS/TLS).
    *   **Изоляция данных:** Векторные индексы и LoRA-адаптеры каждого клиента должны быть строго изолированы.
*   **Защита API:** Rate Limiting (ограничение запросов) для предотвращения DDoS-атак и злоупотреблений.
*   **Обработка пользовательского ввода:** Санитизация и валидация всех входящих данных для предотвращения инъекций в промпт и других атак.
*   **Уязвимости моделей:** Регулярный аудит на предмет уязвимостей, связанных с извлечением тренировочных данных или инъекцией промптов.

---

### 6. План разработки и управление проектом

**Методология:** Гибкая (Agile) с итерациями по 2 недели.

| Этап | Срок (примерно) | Задачи | Ресурсы |
| :--- | :--- | :--- | :--- |
| **Фаза 1: Прототип ядра** | 4-6 недель | 1. Настройка окружения (Python, Docker). <br> 2. Реализация базового RAG пайплайна. <br> 3. Реализация обучения и применения LoRA. <br> 4. Создание простого консольного демо. | Исследователь (Я), VPS с GPU |
| **Фаза 2: Бэкенд и API** | 4 недели | 1. Разработка FastAPI бэкенда. <br> 2. Интеграция с БД (PostgreSQL). <br> 3. Реализация асинхронных задач (Celery). <br> 4. Создание основных API эндпоинтов. | Исследователь (Я) |
| **Фаза 3: Фронтенд и интеграция** | 4 недели | 1. Разработка базовой админ-панели. <br> 2. Интеграция фронтенда с бэкендом. <br> 3. Реализация загрузки файлов и управления ассистентами. | Исследователь (Я) |
| **Фаза 4: Тестирование и полировка** | 3 недели | 1. Всестороннее тестирование. <br> 2. Улучшение UI/UX. <br> 3. Написание документации. <br> 4. Развертывание продакшн-окружения. | Исследователь (Я), возможно, 1-2 тестера |
| **Фаза 5: Написание диссертации** | Параллельно | Оформление результатов и процесса разработки. | Исследователь (Я) |

---

### 7. Стратегия тестирования

*   **Типы тестов:**
    *   **Unit-тесты:** Критичные функции (разбиение текста, формирование промпта) с помощью `pytest`.
    *   **Интеграционные тесты:** Тестирование всего RAG + LoRA пайплайна на небольшом наборе данных.
    *   **Системное тестирование:** Сквозное тестирование через API: создание ассистента, загрузка данных, обучение, запрос.
    *   **Приемочное тестирование (UAT):** Привлечение 2-3 представителей малого бизнеса для тестирования платформы на реальных кейсах.

*   **Критерии успешности:**
    *   **Функциональность:** Все Use Cases реализованы и работают стабильно.
    *   **Качество ответов:** Ответы ассистента оцениваются по шкале (1-5) по релевантности, точности и полезности. Цель - средний балл >4.
    *   **Производительность:** Время ответа ассистента < 3 секунд.
    *   **Удобство использования:** Пользователи могут создать и запустить базового ассистента менее чем за 30 минут.

---

### 8. Документация

*   **Техническая документация:**
    *   **Architecture Decision Record (ADR):** Описание принятых архитектурных решений.
    *   **API Documentation:** Автоматически генерируется FastAPI (Swagger/ReDoc).
    *   **Setup Guide:** `README.md` с инструкцией по развертыванию для разработки.
*   **Пользовательская документация:**
    *   **Getting Started Guide:** Пошаговое руководство по созданию первого ассистента.
    *   **FAQ и Troubleshooting.**

---

### 9. Конкретная область разработки

**Веб-платформа (SaaS).** Проект представляет собой **веб-приложение**, состоящее из:
1.  **Бэкенд-сервиса** (микросервисная или монолитная для MvP архитектура на Python/FastAPI).
2.  **Веб-интерфейса (Frontend)** - одностраничного приложения (SPA) на React/Next.js для управления ассистентами.
3.  **RESTful API** для интеграции с внешними системами (сайты, мессенджеры, CRM).

---

### 10. Детализация архитектуры и БД

**Структура базы данных (PostgreSQL):**

*   **Таблица `users`:** `id`, `email`, `hashed_password`, `created_at`.
*   **Таблица `projects`:** `id`, `user_id` (FK), `name`, `description`, `base_model_name`, `created_at`.
*   **Таблица `documents`:** `id`, `project_id` (FK), `filename`, `file_path`, `status` (`uploaded`, `processing`, `indexed`), `created_at`.
*   **Таблица `lora_adapters`:** `id`, `project_id` (FK), `adapter_path`, `training_logs`, `status` (`training`, `completed`, `failed`), `created_at`.
*   **Таблица `api_keys`:** `id`, `user_id` (FK), `key_hash`, `name`, `created_at`.

**Векторная БД (Chroma/Qdrant):**
*   Коллекция создается для каждого `project_id`.
*   Каждый чанк документа хранится с метаданными: `document_id`, `chunk_index`.

---

### 3. Архитектура проекта

```
+------------------------+     HTTP/gRPC     +-----------------------+
|   Клиент (Streamlit)   | <----------------> |    Backend API (Go)   |
|     Веб-интерфейс      |                    |                       |
+------------------------+                    +-----------------------+
                                                    |  |  |
                    (Управление) (Данные запроса) (Данные для индексации)
                                                    |  |  |
                                    +-----------------------------------+
                                    |               |               |
                                    v               v               v
+------------------------+    +-----------+    +---------+    +------------+
|   Python AI Service    |    | PostgreSQL|    | Chroma  |    |  Файловое  |
|   (Flask/FastAPI)      |    | (Metadata)|    | (Vector)|    |  хранилище |
+------------------------+    +-----------+    +---------+    +------------+
        |
        | (Внутренний вызов)
        v
+-----------------------------------------------------------------------+
|                          AI Core (Python)                             |
| +------------------+  +-----------------------+  +------------------+ |
| |   Embeddings     |  |       LLM Engine      |  |   LoRA Manager   | |
| | (sentence-transformers)|  (Llama 3 8B +  |  |    (PEFT lib)     | |
| | all-MiniLM-L6-v2 |  |    Transformers)     |  |                  | |
| +------------------+  +-----------------------+  +------------------+ |
|          |                   |                        |              |
|          | (Вектор запроса)  | (Контекст + Запрос)    | (Загружает   |
|          +------------------->                        |   адаптер)   |
|                             |                        |              |
|                             +<------------------------+              |
+-----------------------------------------------------------------------+
```

**Объяснение архитектуры:**

1.  **Бэкенд на Go (`Backend API (Go)`):**
    *   Обрабатывает HTTP-запросы от Streamlit и внешние API-вызовы.
    *   Управляет бизнес-логикой: пользователи, проекты, документы, API-ключи.
    *   Сохраняет метаданные в **PostgreSQL**.
    *   Сохраняет файлы в **Файловое хранилище** (локальная папка или S3-совместимое).
    *   Для задач, требующих взаимодействия с Python-экосистемой AI, выступает в роли **клиента** и делает RPC (HTTP/gRPC) вызовы в **Python AI Service**.

2.  **Python AI Service (Flask/FastAPI):**
    *   Отдельный микросервис на Python, который отвечает за всю работу с AI.
    *   Получает задачи от Go-бэкенда.
    *   Выполняет индексацию документов: разбиение на чанки, создание эмбеддингов через `all-MiniLM-L6-v2`, сохранение в **Chroma**.
    *   Управляет жизненным циклом **LoRA-адаптеров** (обучение, загрузка).
    *   Обрабатывает запросы: RAG-поиск в **Chroma** + генерация ответа через **Llama 3** с применением нужного **LoRA-адаптера**.

3.  **Streamlit (Клиент):**
    *   Легковесный веб-интерфейс для демонстрации и управления.
    *   Взаимодействует только с Go-бэкендом через API.

**Почему так?** Это разделение позволяет использовать сильные стороны обоих языков: производительность и надежность Go для бэкенда и богатейшую AI-экосистему Python для работы с моделями.

---

### 4. Используемые технологии

*   **Бэкенд (API и Бизнес-логика):** Go (чистый Go или фреймворк Gin/Echo).
*   **AI Микросервис (Python):** FastAPI (для автоматической документации) или Flask.
*   **Веб-Интерфейс:** Streamlit.
*   **Работа с LLM:** `transformers`, `accelerate`, `bitsandbytes` (4-bit), `torch`.
*   **Модель:** `Llama 3 8B Instruct` (через Hugging Face).
*   **PEFT/LoRA:** Библиотека `peft`.
*   **Эмбеддинги:** `sentence-transformers` / модель `all-MiniLM-L6-v2`.
*   **Векторная БД:** Chroma (в режиме in-memory или персистентном на диске).
*   **Реляционная БД:** PostgreSQL (можно запустить в Docker) или SQLite для максимальной простоты MvP.
*   **Контейнеризация:** Docker, Docker Compose (для изоляции сервисов).
*   **Взаимодействие сервисов:** HTTP/REST API или gRPC (для большей эффективности).

---

### 5. Вопросы безопасности

*   **Аутентификация:** JWT-токены.
*   **Изоляция данных:** Каждый проект — отдельная коллекция в Chroma и отдельная папка с LoRA-адаптерами.
*   **Шифрование:** HTTPS с помощью reverse-proxy (например, Caddy или Nginx в Docker).
*   **Защита API:** Rate Limiting на уровне Go-бэкенда.

---

### 6. План разработки

| Этап | Срок | Задачи |
| :--- | :--- | :--- |
| **Фаза 1: Прототип AI ядра (Python)** | 3 недели | 1. Настройка Python-окружения (Conda/Docker). <br> 2. Запуск Llama 3 8B с 4-bit квантованием. <br> 3. Реализация RAG с Chroma и all-MiniLM-L6-v2. <br> 4. Реализация обучения и применения LoRA. <br> 5. Создание Flask/FastAPI сервиса с 2 эндпоинтами: `/index` и `/chat`. |
| **Фаза 2: Бэкенд на Go** | 3 недели | 1. Разработка Go-бэкенда (Gin/Echo). <br> 2. Реализация логики пользователей и проектов (с SQLite). <br> 3. Написание клиента для вызовов Python AI Service. <br> 4. Создание API эндпоинтов для фронтенда. |
| **Фаза 3: Интеграция и Streamlit** | 2 недели | 1. Разработка UI в Streamlit (управление проектами, загрузка файлов, чат). <br> 2. Интеграция Streamlit с Go-бэкендом. <br> 3. Сквозное тестирование: загрузка документа -> общение. |
| **Фаза 4: Доработка и тесты** | 2 недели | 1. Добавление асинхронной обработки индексации. <br> 2. Улучшение промптов и качества ответов. <br> 3. Написание документации. <br> 4. Создание `docker-compose.yml` для легкого запуска. |

---

### 7. Стратегия тестирования

*   **Unit-тесты:** Go-бэкенд (`go test`), критичные Python-функции (`pytest`).
*   **Интеграционные тесты:** Тестирование взаимодействия Go -> Python AI Service -> Chroma -> Llama.
*   **Критерии успеха:** Качество ответов (оценка >4), время ответа <5 сек.

---

### 8. Документация

*   `README.md`: Инструкция по запуску через `docker-compose up`.
*   **API Documentation:** Автогенерация из Go-бэкенда (Swagger) и Python-сервиса (FastAPI Autodocs).
*   **Architecture Overview:** Описание взаимодействия Go и Python-сервисов.

---

### 9. Конкретная область разработки

**Локально развертываемая веб-платформа (On-Premise MvP).** Комплекс из трех сервисов, запускаемых на одном ПК:
1.  **Go-бэкенд сервис.**
2.  **Python AI микросервис.**
3.  **Streamlit веб-интерфейс.**
4.  **(Опционально) База данных PostgreSQL в Docker.**

---

### 10. Детализация архитектуры и БД

**Структура БД (SQLite для простоты MvP):**
*   Таблицы: `users`, `projects`, `documents`, `lora_adapters`, `api_keys`.

**Структура базы данных (PostgreSQL):**

*   **Таблица `users`:** `id`, `email`, `hashed_password`, `created_at`.
*   **Таблица `projects`:** `id`, `user_id` (FK), `name`, `description`, `base_model_name`, `created_at`.
*   **Таблица `documents`:** `id`, `project_id` (FK), `filename`, `file_path`, `status` (`uploaded`, `processing`, `indexed`), `created_at`.
*   **Таблица `lora_adapters`:** `id`, `project_id` (FK), `adapter_path`, `training_logs`, `status` (`training`, `completed`, `failed`), `created_at`.
*   **Таблица `api_keys`:** `id`, `user_id` (FK), `key_hash`, `name`, `created_at`.

**Векторная БД (Chroma/Qdrant):**
*   Коллекция создается для каждого `project_id`.
*   Каждый чанк документа хранится с метаданными: `document_id`, `chunk_index`.

**Взаимодействие для обработки запроса:**
1.  Пользователь шлет сообщение через Streamlit.
2.  Streamlit отправляет POST запрос на Go-бэкенд `/api/v1/chat`.
3.  Go-бэкенд проверяет API-ключ, находит `project_id`, и делает RPC-вызов на `Python AI Service /chat`.
4.  `Python AI Service` для данного `project_id`:
    *   Ищет в своей памяти загружен ли нужный LoRA-адаптер, если нет — загружает.
    *   Создает эмбеддинг для запроса и ищет релевантные чанки в Chroma (для коллекции `project_id`).
    *   Формирует промпт с контекстом и отправляет в Llama 3 с примененным LoRA-адаптером.
    *   Полученный ответ возвращает в Go-бэкенд.
5.  Go-бэкенд возвращает ответ в Streamlit.

---

### 11. Управление рисками

| Потенциальный риск | Влияние | Вероятность | Стратегия смягчения |
| :--- | :--- | :--- | :--- |
| **Низкое качество ответов** | Высокое | Среднее | Тщательный подбор моделей, эксперименты с разными техниками RAG (HyDE, re-ranking), улучшение промптинга. |
| **Высокая стоимость GPU** | Высокое | Высокое | Использование квантования (4-bit), выбор менее требовательных моделей (7B-8B), оптимизация инференса. |
| **Нехватка времени** | Высокое | Высокое | Четкое следование плану, фокусировка на MvP (минимально жизнеспособный продукт), отказ от non-core фич. |
| **Утечка данных клиента** | Критическое | Низкое | Строгая изоляция, шифрование, регулярные аудиты безопасности. |
| **Сложность интеграции для пользователей** | Среднее | Среднее | Создание подробной документации, предоставление готовых сниппетов кода (JS для виджета, Python для API). |
| **Сложность взаимодействия Go и Python** | Высокое | Низкая | Использование простого и надежного HTTP JSON API. Создание четких контрактов (структур) на обеих сторонах. |
| **Низкая производительность Streamlit** | Низкое | Низкая | Использование Streamlit в чистом режиме отображения, вся логика — в Go-бэкенде. Кэширование тяжелых операций. |

---

### 12. Анализ уязвимостей

*   **Основные векторы атак:**
    *   **Межсервисное взаимодействие:** Go-бэкенд и Python-сервис должны аутентифицировать друг друга (например, с помощью простого секретного токена в заголовках HTTP).
    *   **Прямой доступ к Python-сервису:** Не выставлять порт Python-сервиса наружу. Доступ к нему должен быть только у Go-бэкенда (настроить через Docker network).
    *   **Prompt Injection:** Валидация и санитизация ввода на стороне Go-бэкенда перед отправкой в Python-сервис.
